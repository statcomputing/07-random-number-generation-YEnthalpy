---
title: "Homework 7"
author: "Zehan Yang"
date: "10/23/2020"
header-includes:
- \usepackage{configuration}
output: 
  pdf_document:
    latex_engine: pdflatex
---

\section{Exercise 5.3.1}
\subsection{Problem 1}
\begin{proof}
\begin{equation*}
\begin{aligned}
\because 
&\quad \intinf (2x ^ {\theta - 1} + x ^ {\theta - 1/2})\exp(-x)dx\\
&= 2\intinf x ^ {\theta - 1}\exp(-x)dx + \intinf x ^ {(\theta + 1/2) - 1}\exp(-x)dx\\
&= 2\Gamma(\theta) + \Gamma(\theta + \frac{1}{2})\\
\therefore
&\quad C = \frac{1}{2\Gamma(\theta) + \Gamma(\theta + \frac{1}{2})}.
\end{aligned}
\end{equation*}

Suppose that 
\begin{equation*}
g(x) = \omega_1g_1(x)+\omega_2g_2(x)
= \omega_1\frac{x^{\theta - 1}}{\Gamma(\theta)}\exp(-x)+
\omega_2\frac{x^{(\theta + 1/2) -1}}{\Gamma(\theta+1/2)}\exp(-x), 
\end{equation*}
then we can get that
\begin{equation*}
\begin{aligned}
\omega_1 &= \frac{2\Gamma(\theta)}{2\Gamma(\theta) + 
\Gamma(\theta + \frac{1}{2})},\\
\omega_2 &= \frac{\Gamma(\theta + 
\frac{1}{2})}{2\Gamma(\theta) + \Gamma(\theta + \frac{1}{2})},
\end{aligned}
\end{equation*}
where $\omega_1$ is the weight for the Gamma distribution $g_1$ whose
scale parameter $\beta = 1$ and shape parameter $\alpha = \theta$ and
$\omega_2$ is the weight for the Gamma distribution $g_2$ whose scale
parameter $\beta = 1$ and shape parameter $\alpha = \theta + 1/2$.
\end{proof}
\subsection{Problem 2}
I will choose $\theta = 2.5$, the plots for kernal density and true density are shown below.
```{r}
# Set up initiate values
n <- 10000 
theta <- 2.5
# Function for generate one random sample from g(x) (Newton Method)
r.mixgam <- function(theta, u, control = list(maxit = 1000, 
                                              tol = 1e-5, init = 1)) {
  shape <- c(theta, theta + 0.5)
  weight <- c(2 * gamma(theta), gamma(theta + 0.5)) / 
    (2 * gamma(theta) + gamma(theta + 0.5))
  x <- control$init
  for (i in 1 : control$maxit) {
    upda <- (sum(weight * pgamma(x, shape)) - u) / 
      sum(weight * dgamma(x, shape))
    if (abs(upda) < control$tol) return(x)
    x <- x - upda
    if (i == control$maxit) warning("Max itiration reached")
  }
}
# Generate sample from standard uniform distribution
u <- runif(n) 
# Generate sample from g(x)
kernel <- rep(0, n) 
for (i in 1 : n) {
  kernel[i] <- r.mixgam(theta, u[i])
}
# Calculate the true density
mix.gamma.x <- seq(min(kernel), max(kernel), 0.1)
mix.gamma.y <- rep(0, length(mix.gamma.x))
shape <- c(theta, theta + 0.5)
weight <- c(2 * gamma(theta), gamma(theta + 0.5)) / 
  (2 * gamma(theta) + gamma(theta + 0.5))
for (i in 1 : length(mix.gamma.x)) {
  mix.gamma.y[i] <- sum(weight * dgamma(mix.gamma.x[i], shape))
}
# Draw plot for the kernel density
plot(density(kernel), main = "Desenty Plots", xlab = "X", 
     ylab = "Density", col = "red", lty = 2, lwd = 2)
lines(mix.gamma.y ~ mix.gamma.x, col = "blue", lty = 3, lwd = 2)
# Draw plot for the true density
legend("topright", legend = c("Kernal", "True"), col = c("red", "blue"),
       lty = c(2, 3), lwd = c(2, 2))
```

\subsection{Problem 3}
First, we state the following fact that
\begin{equation}\label{eq1}
 x^{\theta-1/2}\exp(-x) \le \sqrt{4+x}x^{\theta-1}\exp(-x) \le (2x^{\theta-1}+x^{\theta-1/2})\exp(-x).
\end{equation}
Next, Suppose $f(x) = C_1 \sqrt{4+x}x^{\theta-1}\exp(-x)$ 
and $h(x) = \frac{1}{\Gamma(\theta+1/2)}x^{\theta-1/2}\exp(-x)$ 
where $h(x)$ is the PDF for Gamma Distribution with 
shape parameter $\alpha = \theta+1/2$ and scale parameter $\beta = 1$.
Then , based on \eqref{eq1}, we have $C_1<\frac{1}{\Gamma(\theta+1/2)}$ and
\begin{equation*}
\begin{aligned}
f(x) &\le \frac{1}{\Gamma(\theta+1/2)}
\sqrt{4+x}x^{\theta-1}\exp(-x) \\
&\le M\cdot\frac{1}{\Gamma(\theta+1/2) + 2\Gamma(\theta)}(2x^{\theta-1}+x^{\theta-1/2})\exp(-x)\\
&= M\cdot g(x),\\
&where,~M=\frac{\Gamma(\theta+1/2) + 2\Gamma(\theta)}{\Gamma(\theta+1/2)}.
\end{aligned}
\end{equation*}
Also, we have
\begin{equation*}
\begin{aligned}
f(x) &\ge \frac{1}{\Gamma(\theta+1/2) + 2\Gamma(\theta)}\sqrt{4+x}x^{\theta-1}\exp(-x), \\
\frac{f(x)}{M\cdot g(x)} &\ge \frac{\Gamma(\theta+1/2)}{\Gamma(\theta+1/2) + 2\Gamma(\theta)} \frac{\sqrt{4+x}}{2+x^{-1/2}}.
\end{aligned}
\end{equation*}
The same as Problem 2, I choose $\theta = 2.5$.
```{r}
# Function for generate n random samples from f(x)
r.mixgam.2 <- function(n, theta, control = list(maxit = 1000)){
  x <- rep(0, n)
  j <- 1
  while(j < n) {
    for (i in 1 : control$maxit) {
    y <- r.mixgam(theta, runif(1))
    u <- runif(1)
    crit <- (gamma(theta + 0.5) * sqrt(4 + y)) / 
      ((2 * gamma(theta) + gamma(theta + 0.5)) * (2 + y ^ (1/2)))
      if (u <= crit){
        x[j] = y
        j <- j + 1
        break
      }
      if (i == control$maxit){
        j <- j
      }
    }
  }
  return(x)
}
# Calculate the true density
fx <- function(x, ...){
  sqrt(4 + x) * x ^ (theta - 1) * exp(-x)
}
c1 <- 1 / integrate(fx, 0, Inf)$value
mix.gamma.x <- seq(min(kernel), max(kernel), 0.1)
mix.gamma.y <- fx(mix.gamma.x) * c1
# Draw plot for the kernel density
kernel <- r.mixgam.2(10000, 2.5)
plot(density(kernel), main = "Desenty Plots", xlab = "X", 
     ylab = "Density", col = "red", lty = 2, lwd = 2)
lines(mix.gamma.y ~ mix.gamma.x, col = "blue", lty = 3, lwd = 2)
# Draw plot for the true density
legend("topright", legend = c("Kernal", "True"), col = c("red", "blue"),
       lty = c(2, 3), lwd = c(2, 2))
```
\section{Exercise 6.3.1}
Suppose $\{X_i\}_{i=1}^n$ follows the target Mixed Normal distribution.
Based on the assumptions, the posterior density of $(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \omega)$ should be
\begin{equation}
\begin{aligned}
&\quad q(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \omega|X_1,...,X_n) \\
&= \prod_{i=1}^n f_1(X_i|\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \omega)
\cdot f_2(\mu_1) \cdot f_2(\mu_2) 
\cdot f_3(\sigma_1^2) \cdot f_3(\sigma_2^2),
\end{aligned}
\end{equation}
where $f_1(.)$ is the conditional distribution for the Mixed Normal distribution; 
$f_2(.)$ is the PDF for Normal distribution with $\mu = 0$ and $\sigma^2 = 100$ 
and $f_3(.)$ is the PDF for Inverse-Gamma distribution with shape
parameter $\alpha = 0.5$ and scale parameter $\beta = 10$.
I will generate the data first.
```{r}
n <- 1000
mu <- c(7, 10)
sigma <- c(1, 1)
weight <- 0.5
u <- rbinom(n, prob = weight, size = 1)
x <- rnorm(n, ifelse(u == 1, mu[1], mu[2]), 
           ifelse(u == 1, sigma[1], sigma[2]))
```

The log-posterior density should be easy to define.
```{r}
logpost <- function(theta, x) {
  delta <- theta[1]
  mu1 <- theta[2]; mu2 <- theta[3]
  sigma2.1 <- theta[4]; sigma2.2 <- theta[5]
  return(sum(log(delta * dnorm(x, mu1, sqrt(sigma2.1)) + (1 - delta) * dnorm(x, mu2, sqrt(sigma2.2))))
         + log(dnorm(mu1, 0, 10)) + log(dnorm(mu2, 0, 10)) - 1.5 * log((sigma2.1 * sigma2.2)) 
         - 10 * (sigma2.1 ^ (-1) + sigma2.2 ^ (-1)))
}
```
Then, an MCMC based on Gibbs method using 'arms' algorithm in the package 'HI' is shown below.
```{r}
mymcmc <- function(niter, thetaInit, x, nburn = 100) {
  p <- length(thetaInit)
  theta.old <- thetaInit
  logFC <- function(th, idx) {
    theta <- theta.old
    theta[idx] <- th
    logpost(theta, x)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  for (i in 2 : niter) {
    for (j in 1 : p) {
      out[i, j] <- theta.old[j] <- 
        HI::arms(theta.old[j], logFC,
                 function(x, idx) ((x > -100) * (x < 100)), 
                 1, idx = j)
    }
  }
  return(out[-(1 : nburn), ])
}
```
Finally, plot the histograms for all parameters.






